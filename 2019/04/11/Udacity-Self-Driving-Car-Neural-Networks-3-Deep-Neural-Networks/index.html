<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning, Deep Learning, Self Driving, Linux">










<meta name="description" content="Logistic ClassifierA linear classifier, it takes in the inputs and applies the linear function to them to generate the predictions (classifications). Training, validation and test set Training set: Da">
<meta property="og:type" content="article">
<meta property="og:title" content="Udacity Self-Driving Car - Neural Networks: 3. Deep Neural Networks">
<meta property="og:url" content="http://yoursite.com/2019/04/11/Udacity-Self-Driving-Car-Neural-Networks-3-Deep-Neural-Networks/index.html">
<meta property="og:site_name" content="皇甫二小">
<meta property="og:description" content="Logistic ClassifierA linear classifier, it takes in the inputs and applies the linear function to them to generate the predictions (classifications). Training, validation and test set Training set: Da">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://github.com/DongzhenHuangfu/pictures-for-blog/raw/master/Udacity_self_driving_car_neural_networks/3_1_pooling.png">
<meta property="og:image" content="https://github.com/DongzhenHuangfu/pictures-for-blog/raw/master/Udacity_self_driving_car_neural_networks/3_2_maxpooling.png">
<meta property="og:updated_time" content="2019-04-11T08:48:57.123Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Udacity Self-Driving Car - Neural Networks: 3. Deep Neural Networks">
<meta name="twitter:description" content="Logistic ClassifierA linear classifier, it takes in the inputs and applies the linear function to them to generate the predictions (classifications). Training, validation and test set Training set: Da">
<meta name="twitter:image" content="https://github.com/DongzhenHuangfu/pictures-for-blog/raw/master/Udacity_self_driving_car_neural_networks/3_1_pooling.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/04/11/Udacity-Self-Driving-Car-Neural-Networks-3-Deep-Neural-Networks/">





  <title>Udacity Self-Driving Car - Neural Networks: 3. Deep Neural Networks | 皇甫二小</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">皇甫二小</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">带着赤子的骄傲</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/11/Udacity-Self-Driving-Car-Neural-Networks-3-Deep-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wyatt Huangfu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="皇甫二小">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Udacity Self-Driving Car - Neural Networks: 3. Deep Neural Networks</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-11T10:56:41+02:00">
                2019-04-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Logistic-Classifier"><a href="#Logistic-Classifier" class="headerlink" title="Logistic Classifier"></a>Logistic Classifier</h2><p>A linear classifier, it takes in the inputs and applies the linear function to them to generate the predictions (classifications).</p>
<h2 id="Training-validation-and-test-set"><a href="#Training-validation-and-test-set" class="headerlink" title="Training, validation and test set"></a>Training, validation and test set</h2><ul>
<li>Training set: Data set for training.  </li>
<li>Validation set: Data set for judging the model during the training process.  </li>
<li>Test set: Data set for test the model after the training process.  </li>
</ul>
<h2 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross-Validation"></a>Cross-Validation</h2><p>See <a href="https://towardsdatascience.com/cross-validation-70289113a072" target="_blank" rel="noopener">here</a></p>
<h2 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent (SGD)"></a>Stochastic Gradient Descent (SGD)</h2><p>Estimate with a random, small part of the training data (1-1000). Compute the lost and the derivative for this sample and update the parameters with this derivative as the direction for the gradient descent.</p>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p>Keep a running average of the gradient and use that running average instead of the direction of the current batch of the data.  </p>
<ul>
<li>Better convergence.</li>
</ul>
<h4 id="Hyper-parameters"><a href="#Hyper-parameters" class="headerlink" title="Hyper-parameters"></a>Hyper-parameters</h4><ul>
<li>Initial learning rate  </li>
<li>Learning rate decay  </li>
<li>Momentum  </li>
<li>Batch size  </li>
<li>Weight initialization  </li>
</ul>
<p>Reduce the learning rate can be helpful if the thing goes not well!  </p>
<h4 id="ADAGRAD"><a href="#ADAGRAD" class="headerlink" title="ADAGRAD"></a>ADAGRAD</h4><p>A optimized SGD, which chooses the initial learning rate, learning rate decay and momentum for you.  </p>
<h2 id="Mini-Batching"><a href="#Mini-Batching" class="headerlink" title="Mini-Batching"></a>Mini-Batching</h2><p>A technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.  </p>
<h4 id="Combination-with-SGD"><a href="#Combination-with-SGD" class="headerlink" title="Combination with SGD"></a>Combination with SGD</h4><p> Randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you’re performing SGD with each batch.  </p>
<h2 id="Epochs"><a href="#Epochs" class="headerlink" title="Epochs"></a>Epochs</h2><p>An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data.  </p>
<h2 id="Example-Classify-the-letters-in-the-MNIST-database"><a href="#Example-Classify-the-letters-in-the-MNIST-database" class="headerlink" title="Example: Classify the letters in the MNIST database"></a>Example: Classify the letters in the MNIST database</h2><pre><code># import the data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&quot;.&quot;, one_hot=True, reshape=False)

# define the learning parameters
import tensorflow as tf

learning_rate = 0.001
training_epochs = 20
batch_size = 128  # Decrease batch size if you don&#39;t have enough memory
display_step = 1

n_input = 784  # MNIST data input (img shape: 28*28)
n_classes = 10  # MNIST total classes (0-9 digits)

# define the size of the hidden layer
n_hidden_layer = 256 # layer number of features

# define the weights and biases
weights = { 
    &#39;hidden_layer&#39;: tf.Variable(tf.random_normal([n_input, n_hidden_layer])),
    &#39;out&#39;: tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))
}
biases = {
    &#39;hidden_layer&#39;: tf.Variable(tf.random_normal([n_hidden_layer])),
    &#39;out&#39;: tf.Variable(tf.random_normal([n_classes]))
}

# define the inputs and outputs
x = tf.placeholder(&quot;float&quot;, [None, 28, 28, 1])
y = tf.placeholder(&quot;float&quot;, [None, n_classes])
x_flat = tf.reshape(x, [-1, n_input])

# define the neural network
# Hidden layer with RELU activation
layer_1 = tf.add(tf.matmul(x_flat, weights[&#39;hidden_layer&#39;]), biases[&#39;hidden_layer&#39;])
layer_1 = tf.nn.relu(layer_1)
# Output layer with linear activation
logits = tf.add(tf.matmul(layer_1, weights[&#39;out&#39;]), biases[&#39;out&#39;])

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)

# Initializing the variables
init = tf.global_variables_initializer()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
    # Training cycle
    for epoch in range(training_epochs):
        total_batch = int(mnist.train.num_examples/batch_size)
        # Loop over all batches
        for i in range(total_batch):
            batch_x, batch_y = mnist.train.next_batch(batch_size)
            # Run optimization op (backprop) and cost op (to get loss value)
            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})
</code></pre><p>Note: The code <code>minst.train.next_batch()</code> returns a subset of the training data.</p>
<h2 id="Save-and-restore-the-TensorFlow-Models"><a href="#Save-and-restore-the-TensorFlow-Models" class="headerlink" title="Save and restore the TensorFlow Models"></a>Save and restore the TensorFlow Models</h2><h4 id="Save-Variables"><a href="#Save-Variables" class="headerlink" title="Save Variables"></a>Save Variables</h4><pre><code>import tensorflow as tf

# The file path to save the data
save_file = &#39;./model.ckpt&#39;

# Two Tensor Variables: weights and bias
weights = tf.Variable(tf.truncated_normal([2, 3]))
bias = tf.Variable(tf.truncated_normal([3]))

# Class used to save and/or restore Tensor Variables
saver = tf.train.Saver()

with tf.Session() as sess:
    # Initialize all the Variables
    sess.run(tf.global_variables_initializer())

    # Show the values of weights and bias
    print(&#39;Weights:&#39;)
    print(sess.run(weights))
    print(&#39;Bias:&#39;)
    print(sess.run(bias))

    # Save the model
    saver.save(sess, save_file)
</code></pre><h4 id="Load-Variables"><a href="#Load-Variables" class="headerlink" title="Load Variables"></a>Load Variables</h4><pre><code># Remove the previous weights and bias
tf.reset_default_graph()

# Two Variables: weights and bias
weights = tf.Variable(tf.truncated_normal([2, 3]))
bias = tf.Variable(tf.truncated_normal([3]))

# Class used to save and/or restore Tensor Variables
saver = tf.train.Saver()

with tf.Session() as sess:
    # Load the weights and bias
    saver.restore(sess, save_file)

    # Show the values of weights and bias
    print(&#39;Weight:&#39;)
    print(sess.run(weights))
    print(&#39;Bias:&#39;)
    print(sess.run(bias))
</code></pre><h4 id="Save-Model"><a href="#Save-Model" class="headerlink" title="Save Model"></a>Save Model</h4><pre><code># Remove previous Tensors and Operations
tf.reset_default_graph()

from tensorflow.examples.tutorials.mnist import input_data
import numpy as np

learning_rate = 0.001
n_input = 784  # MNIST data input (img shape: 28*28)
n_classes = 10  # MNIST total classes (0-9 digits)

# Import MNIST data
mnist = input_data.read_data_sets(&#39;.&#39;, one_hot=True)

# Features and Labels
features = tf.placeholder(tf.float32, [None, n_input])
labels = tf.placeholder(tf.float32, [None, n_classes])

# Weights &amp; bias
weights = tf.Variable(tf.random_normal([n_input, n_classes]))
bias = tf.Variable(tf.random_normal([n_classes]))

# Logits - xW + b
logits = tf.add(tf.matmul(features, weights), bias)

# Define loss and optimizer
cost = tf.reduce_mean(\
    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\
    .minimize(cost)

# Calculate accuracy
correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
import math

save_file = &#39;./train_model.ckpt&#39;
batch_size = 128
n_epochs = 100

saver = tf.train.Saver()

# Launch the graph
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    # Training cycle
    for epoch in range(n_epochs):
        total_batch = math.ceil(mnist.train.num_examples / batch_size)

        # Loop over all batches
        for i in range(total_batch):
            batch_features, batch_labels = mnist.train.next_batch(batch_size)
            sess.run(
                optimizer,
                feed_dict={features: batch_features, labels: batch_labels})

        # Print status for every 10 epochs
        if epoch % 10 == 0:
            valid_accuracy = sess.run(
                accuracy,
                feed_dict={
                    features: mnist.validation.images,
                    labels: mnist.validation.labels})
            print(&#39;Epoch {:&lt;3} - Validation Accuracy: {}&#39;.format(
                epoch,
                valid_accuracy))

    # Save the model
    saver.save(sess, save_file)
    print(&#39;Trained Model Saved.&#39;)
</code></pre><h4 id="Load-Model"><a href="#Load-Model" class="headerlink" title="Load Model"></a>Load Model</h4><pre><code>saver = tf.train.Saver()

# Launch the graph
with tf.Session() as sess:
    saver.restore(sess, save_file)

    test_accuracy = sess.run(
        accuracy,
        feed_dict={features: mnist.test.images, labels: mnist.test.labels})

print(&#39;Test Accuracy: {}&#39;.format(test_accuracy))
</code></pre><h2 id="Naming-Error"><a href="#Naming-Error" class="headerlink" title="Naming Error"></a>Naming Error</h2><p>Error description:<br>InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match.</p>
<p>Example:<br>    import tensorflow as tf</p>
<pre><code># Remove the previous weights and bias
tf.reset_default_graph()

save_file = &#39;model.ckpt&#39;

# Two Tensor Variables: weights and bias
weights = tf.Variable(tf.truncated_normal([2, 3]))
bias = tf.Variable(tf.truncated_normal([3]))

saver = tf.train.Saver()

# Print the name of Weights and Bias
print(&#39;Save Weights: {}&#39;.format(weights.name)) # Save Weights: Variable:0
print(&#39;Save Bias: {}&#39;.format(bias.name)) # Save Bias: Variable_1:0

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    saver.save(sess, save_file)

# Remove the previous weights and bias
tf.reset_default_graph()

# Two Variables: weights and bias
bias = tf.Variable(tf.truncated_normal([3])) # Load Weights: Variable_1:0
weights = tf.Variable(tf.truncated_normal([2, 3])) # Load Bias: Variable:0

saver = tf.train.Saver()

# Print the name of Weights and Bias
print(&#39;Load Weights: {}&#39;.format(weights.name))
print(&#39;Load Bias: {}&#39;.format(bias.name))

with tf.Session() as sess:
    # Load the weights and bias - ERROR
    saver.restore(sess, save_file)
</code></pre><p>Reason: TensorFlow use a string identifier “name” for Tensors and Operations. If it is not given, TensorFlow will create one automatically.<br>You can define the “name” manually:<br>    import tensorflow as tf</p>
<pre><code>tf.reset_default_graph()

save_file = &#39;model.ckpt&#39;

# Two Tensor Variables: weights and bias
weights = tf.Variable(tf.truncated_normal([2, 3]), name=&#39;weights_0&#39;)
bias = tf.Variable(tf.truncated_normal([3]), name=&#39;bias_0&#39;)

saver = tf.train.Saver()

# Print the name of Weights and Bias
print(&#39;Save Weights: {}&#39;.format(weights.name))
print(&#39;Save Bias: {}&#39;.format(bias.name))

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    saver.save(sess, save_file)

# Remove the previous weights and bias
tf.reset_default_graph()

# Two Variables: weights and bias
bias = tf.Variable(tf.truncated_normal([3]), name=&#39;bias_0&#39;)
weights = tf.Variable(tf.truncated_normal([2, 3]) ,name=&#39;weights_0&#39;)

saver = tf.train.Saver()

# Print the name of Weights and Bias
print(&#39;Load Weights: {}&#39;.format(weights.name))
print(&#39;Load Bias: {}&#39;.format(bias.name))

with tf.Session() as sess:
    # Load the weights and bias - No Error
    saver.restore(sess, save_file)

print(&#39;Loaded Weights and Bias successfully.&#39;)
</code></pre><h2 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h2><p><script type="math/tex">\ell_2</script> regularization add the $\ell_2$ norm of the weight multiplied with a little value to punish too large weights, as the equation \ref{L2} shows.</p>
<p>\begin{equation}<br>l’ = l + \frac{1}{2}\beta\lVert w \rVert^2_2<br>\label{L2}<br>\end{equation}</p>
<p>Where $l’$ is the new loss, $l$ is the old loss and $w$ is the weight.  </p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>The Dropout unit can be inserted in the network structure, it works like a layer. The layer takes in the input of the former layer, randomly sets some of them as zero and enlarges the other correspondingly. For example if the half of the input values are set as zero, the others will be scaled by a factor $2$. It makes the model “not so concentrate” on a specific feature and become more universal.<br>When evaluating the model, the dropout rate should be set as $1$, which means no dropout will appear. It is because the Dropout layer only works on the training process.<br>See <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout" target="_blank" rel="noopener">here</a>.  </p>
<h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><p><code>tf.nn.dropout()</code>  </p>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><pre><code>keep_prob = tf.placeholder(tf.float32) # probability to keep units

hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])
hidden_layer = tf.nn.relu(hidden_layer)
hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)

logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])
</code></pre><p>Where:  </p>
<ol>
<li>hidden_layer: the tensor to which you would like to apply dropout.  </li>
<li>keep_prob: the probability of keeping (i.e. not dropping) any given unit. (recommend 0.5)  </li>
</ol>
<h2 id="Pooling-Max-pooling-method"><a href="#Pooling-Max-pooling-method" class="headerlink" title="Pooling/Max-pooling method"></a>Pooling/Max-pooling method</h2><p>The pooling method is a sample-based discretization process which is well-known as a method for resolving over-fitting problems by abstracting the information from the input and reduce the calculation cost by reducing the number of the parameters for learning, as the Figure shows:</p>
<p><img width="40%" src="https://github.com/DongzhenHuangfu/pictures-for-blog/raw/master/Udacity_self_driving_car_neural_networks/3_1_pooling.png"></p>
<p>The pooling unit has an user defined size and will slide on the input data with an user defined step size (stride). Each time the unit moves, it will concentrate the information it takes in, for example, the max-pooling method will output the maximum value of the input. The Figure shows the basic idea of a max-pooling method with size $2 \times 2$ and stride $2$:</p>
<p><img width="40%" src="https://github.com/DongzhenHuangfu/pictures-for-blog/raw/master/Udacity_self_driving_car_neural_networks/3_2_maxpooling.png"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/09/Udacity-Self-driving-Car-Neural-Networks-2-Introduction-to-TensorFlow/" rel="next" title="Udacity Self-driving Car - Neural Networks: 2. Introduction to TensorFlow">
                <i class="fa fa-chevron-left"></i> Udacity Self-driving Car - Neural Networks: 2. Introduction to TensorFlow
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Wyatt Huangfu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Classifier"><span class="nav-number">1.</span> <span class="nav-text">Logistic Classifier</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-validation-and-test-set"><span class="nav-number">2.</span> <span class="nav-text">Training, validation and test set</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cross-Validation"><span class="nav-number">3.</span> <span class="nav-text">Cross-Validation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stochastic-Gradient-Descent-SGD"><span class="nav-number">4.</span> <span class="nav-text">Stochastic Gradient Descent (SGD)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Momentum"><span class="nav-number">4.0.1.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hyper-parameters"><span class="nav-number">4.0.2.</span> <span class="nav-text">Hyper-parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ADAGRAD"><span class="nav-number">4.0.3.</span> <span class="nav-text">ADAGRAD</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#Mini-Batching"><span class="nav-number">5.</span> <span class="nav-text">Mini-Batching</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Combination-with-SGD"><span class="nav-number">5.0.1.</span> <span class="nav-text">Combination with SGD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Epochs"><span class="nav-number">6.</span> <span class="nav-text">Epochs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-Classify-the-letters-in-the-MNIST-database"><span class="nav-number">7.</span> <span class="nav-text">Example: Classify the letters in the MNIST database</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Save-and-restore-the-TensorFlow-Models"><span class="nav-number">8.</span> <span class="nav-text">Save and restore the TensorFlow Models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Save-Variables"><span class="nav-number">8.0.1.</span> <span class="nav-text">Save Variables</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Load-Variables"><span class="nav-number">8.0.2.</span> <span class="nav-text">Load Variables</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Save-Model"><span class="nav-number">8.0.3.</span> <span class="nav-text">Save Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Load-Model"><span class="nav-number">8.0.4.</span> <span class="nav-text">Load Model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Naming-Error"><span class="nav-number">9.</span> <span class="nav-text">Naming Error</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L2-Regularization"><span class="nav-number">10.</span> <span class="nav-text">L2 Regularization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout"><span class="nav-number">11.</span> <span class="nav-text">Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Code"><span class="nav-number">11.0.1.</span> <span class="nav-text">Code</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Example"><span class="nav-number">11.0.2.</span> <span class="nav-text">Example</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pooling-Max-pooling-method"><span class="nav-number">12.</span> <span class="nav-text">Pooling/Max-pooling method</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wyatt Huangfu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
